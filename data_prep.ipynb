{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "source": [
    "# Data prep for Pump it Up project\n",
    "\n",
    "Back in the pcda class we did quite a bit of data prep work using R. Rarely is data ready for statistical/ML predictive modeling in its raw state. In this notebook we will do a few common data prep tasks in Python. This is not meant to be totally comprehensive and I make a bunch of big assumptions and take some shortcuts so that we can move on to modeling. A few of things we'll do, include:\n",
    "\n",
    "* reading in the raw data from csv files - pandas\n",
    "* automated EDA - pandas profiling and sweetviz\n",
    "* manual EDA - pandas and matplotlib/Seaborn\n",
    "* data type conversions - pandas\n",
    "* variable dropping - pandas\n",
    "* factor lumping - a port of R forcats package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# To auto-reload modules in jupyter notebook (so that changes in files *.py doesn't require manual reloading):\n",
    "# https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Import commonly used libraries and magic command for inline plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Read in raw data\n",
    "\n",
    "Let's do the following:\n",
    "\n",
    "* read in train_x and train_y to pandas dataframes,\n",
    "* merge the data frames into a single training dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_x = pd.read_csv(\"./data/raw/train_x.csv\", parse_dates=['date_recorded'])\n",
    "train_y = pd.read_csv(\"./data/raw/train_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join the two dataframes on the `id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_x, train_y, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial EDA\n",
    "\n",
    "As always, let's check out the structure of the dataframes and scan the values a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[:5, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[:5, 15 :30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[:5, 30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will likely be dropping one or more columns before trying to fit models, let's create a list of all the column names and then we can remove some as we go. We can use `train_df.columns` to get an `Index` object whose values are the column names. To get a list version, use the `tolist` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = train_df.columns.tolist()\n",
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop `date_recorded` and `recorded_by` as they seem irrelevant for a predictive model. What are some different ways to remove an item from a list in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove a single element\n",
    "# cols_to_keep.remove('date_recorded')\n",
    "\n",
    "# To remove multiple elements, create a list of cols to drop and then use\n",
    "# a list comprehension to get a list of the ones to keep\n",
    "to_drop = ['date_recorded', 'recorded_by']\n",
    "\n",
    "cols_to_keep = [c for c in cols_to_keep if c not in to_drop]\n",
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated EDA\n",
    "\n",
    "In R, we used a package called skimr to do quick, automated, basic EDA. Surely similar tools must exist for Python? A [little digging turned up several tools](https://github.com/mstaniak/autoEDA-resources). We'll look at two here - pandas-profiling and sweetviz.\n",
    "\n",
    "### ydata-profiling (formerly pandas-profiling)\n",
    "\n",
    "This has been around for a while and quickly gives you a much deeper look at pandas dataframes than you'd get from the the `info` and `describe` methods. The main site is at https://github.com/ydataai/ydata-profiling. Earlier this year, the very popular **pandas-profiling** package became **ydata-profiling** and has a much broader goal than the original.\n",
    "\n",
    "If you want to install it and try it out, I'd suggest using conda to install it via:\n",
    "\n",
    "    conda install -c conda-forge ydata-profiling\n",
    "    \n",
    "I haven't had time to test out this new version. Here's a screenshot from the old version and I've left the old report in the `output/` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pandas profiling output](images/pandas_profiling_output.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweetviz\n",
    "\n",
    "Another impressive automated EDA packack is [Sweetviz](https://github.com/fbdesignpro/sweetviz. Like pandas-profiling, it creates an interactive HTML based report. It includes some features aimed at predictive modeling such as explicit analysis of the target variable and advanced correlation analysis. Some relevant references on Python tools for correlation analysis include:\n",
    "\n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n",
    "\n",
    "To install sweetviz:\n",
    "\n",
    "    pip install sweetviz\n",
    "    \n",
    "I've decided not to install it this year as development on the package seems to have slowed or stopped. Again, I've kept the old report around so you can see it. It's in the `output/` folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report = sweetviz.analyze(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report.show_html(\"output/sweetviz_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data dictionary and the dataframe snooping above, a few observations:\n",
    "\n",
    "* the target variable, `status_group`, has three levels\n",
    "* many fields are non-numeric\n",
    "* several fields have a large proportion of zero values - what is the meaning (missing or zero)?\n",
    "* some of the numeric fields are actually categorical data (`region_code` and `district_code`)\n",
    "* some of the fields seem to be related in a hierarchical fashion (`extraction_type`, `extraction_type_group`, `extraction_type_class`)\n",
    "* Seems like we could eliminate a number of variables before model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable - status_group\n",
    "\n",
    "While this information is available in the automated EDA reports, just demonstrating some basic pandas techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['status_group']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['status_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the numeric variables\n",
    "Sometimes it's useful to be able to select columns by data type. For example to get a list of numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `id` field is just an identifier and won't be used in any models but we'll leave it in the dataframe for now.\n",
    "\n",
    "The `amount_tsh` field has about 70% zeroes and not exactly sure what this means. \n",
    "\n",
    "The `gps_height` field has about 30% zeroes. Is this really sea level or missing data?\n",
    "\n",
    "The `latitude` and `longitude` fields are fully populated. Each degree is about 70 miles.\n",
    "\n",
    "The `num_private` has no definition posted and is 99% zeroes. For the non-zeroes, let's see how the target variable is distributed. Note the use of the `value_counts` function to avoid having to do a group by and using `normalize=True` to get percentages instead of counts. See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.num_private > 0]['status_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher percentage of functional wells in the subset of data having `num_private > 0`. Let's keep it around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `region_code` and `district_code` are categorical. Let's convert their type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"region_code\"] = train_df[\"region_code\"].astype(\"category\")\n",
    "train_df[\"district_code\"] = train_df[\"district_code\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `population` field has 36% zeroes. This is defined as the population around the well. Not sure exactly what the 0's mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('construction_year')['status_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_by_year = pd.crosstab(train_df['construction_year'], train_df['status_group'], normalize='index')\n",
    "status_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, looks like newer wells are more likely to be functioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use pandas `plot` function to quickly generate plot for this dataframe. It's just using matplotlib under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_by_year[status_by_year.index > 0].plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the categorical variables\n",
    "Categorical variables provide challenges in building predictive models in sklearn. For example, when we built regression models in R, we simply included categorical variables in our regression formulas and R automatically created dummy (binary indicator) variables. In sklearn, we have to do this work ourself. Similarly, the implementation of random forests in the R randomForest package can handle categorical variables with no preprocessing necessary.\n",
    "Thankfully, sklearn has a number of tools to help you properly encode such variables for different models and algorithms. We will explore this in more detail in the next notebook when we build models. For now, check out the following blog post that address this topic.\n",
    "\n",
    "* https://pbpython.com/categorical-encoding.html from Practical Business Python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the EDA reports to look for:\n",
    "\n",
    "* obvious variables to drop due to things like little to no variance in the variable values.\n",
    "* variables that might be candidates for lumping due to huge numbers of unique values\n",
    "* categorical variables that are related to each other - e.g. hierarchical relationships such as `extraction_type`, `extraction_type_group`, `extraction_type_class`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>SIDEBAR:</b> How does one do dplyr type chaining with pandas?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring categorical data with pandas, sometimes I find myself longing for the R package, dplyr. For example how to do this dplyr type analysis in pandas?\n",
    "\n",
    "    train %>% \n",
    "      group_by(payment) %>% \n",
    "      summarize(\n",
    "        num_installs = n(),\n",
    "        pct_functional = sum(status_group == 'functional') / num_installs\n",
    "      ) %>% \n",
    "      filter(num_installs > 100) %>%\n",
    "      arrange(desc(pct_functional))\n",
    "\n",
    "To get:\n",
    "\n",
    "    payment  num_installs   pct_functional\n",
    "    pay annually\t3642\t0.7523339\t\n",
    "    pay per bucket\t8985\t0.6777963\t\n",
    "    ...\n",
    "    \n",
    "More generally, how to visualize the breakdown of the categorical target variable within levels of some categorical predictor? Is it just a heat map? Or entropy or gini by level in comparison (perhaps weighted by number of observations in the level) to entropy/gini of target?\n",
    "\n",
    "Down the rabbit hole with the following reddit post. Good stuff found in there and a generally useful discussion. More work going on in adding tidyverse approaches to pandas Python.\n",
    "\n",
    "* https://www.reddit.com/r/datascience/comments/ltkt9s/r_is_far_superior_to_python_for_data_manipulation/\n",
    "* https://tomaugspurger.github.io/modern-1-intro\n",
    "* https://tomaugspurger.github.io/method-chaining - this is really good\n",
    "* https://stmorse.github.io/journal/tidyverse-style-pandas.html - and so is this\n",
    "* [R to Python data wrangling snippets](https://gist.github.com/conormm/fd8b1980c28dd21cfaf6975c86c74d07)\n",
    "* https://www.tidymodels.org/\n",
    "* https://pyjanitor.readthedocs.io/\n",
    "* https://towardsdatascience.com/the-unreasonable-effectiveness-of-method-chaining-in-pandas-15c2109e3c69\n",
    "* https://github.com/machow/siuba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>END OF SIDEBAR</b> Be careful before checking out the reddit discussion above as it can become a time sink (though an interesting and useful one).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic categorical variables\n",
    "\n",
    "There are several categorical variables that are geographic in nature.\n",
    "\n",
    "* `wpt_name` - waterpoint name (if exists)\n",
    "* `basin` - Geographic water basin\n",
    "* `subvillage` - Geographic location\n",
    "* `region` - Geographic location\n",
    "* `region_code` - Geographic location (coded)\n",
    "* `district_code` - Geographic location (coded)\n",
    "* `lga` - Geographic location\n",
    "* `ward` - Geographic location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cols = ['wpt_name', 'basin', 'subvillage', 'region', 'region_code', 'district_code', 'lga', 'ward']\n",
    "\n",
    "train_df.loc[:, geo_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop `region_code` and just use `region`. Let's also remove `wpt_name`, `subvillage` and `ward` as too detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep.remove('region_code')\n",
    "cols_to_keep.remove('wpt_name')\n",
    "cols_to_keep.remove('subvillage')\n",
    "cols_to_keep.remove('ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.loc[:, cols_to_keep]\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical variables\n",
    "\n",
    "* `scheme_name`\n",
    "  `scheme_management`\n",
    "  \n",
    "* `extraction_type`\n",
    "  `extraction_type_group`\n",
    "  `extraction_type_class`\n",
    "\n",
    "* `management`\n",
    "  `management_group`\n",
    " \n",
    "* `payment`\n",
    "  `payment_type`\n",
    " \n",
    "* `water_quality`\n",
    "  `quality_group`\n",
    " \n",
    "* `quantity`\n",
    "  `quantity_group`\n",
    " \n",
    "* `source`\n",
    "  `source_type`\n",
    "  `source_class`\n",
    " \n",
    "* `waterpoint_type`\n",
    "  `waterpoint_type_group`\n",
    " \n",
    "We should explore these more deeply, but for now, let's just include the most aggregate version of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs_to_drop = ['extraction_type', 'extraction_type_group', 'management', 'payment', 'water_quality',\n",
    "               'quantity', 'source', 'source_type', 'waterpoint_type', 'scheme_name']\n",
    "\n",
    "cols_to_keep = [c for c in cols_to_keep if c not in aggs_to_drop]\n",
    "cols_to_keep\n",
    "\n",
    "train_df = train_df.loc[:, cols_to_keep]\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "Will just fill missing data in the categorical fields with \"missing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['funder'] = train_df['funder'].fillna(\"missing\")\n",
    "train_df['installer'] = train_df['installer'].fillna(\"missing\")\n",
    "train_df['public_meeting'] = train_df['public_meeting'].fillna(\"missing\")\n",
    "train_df['scheme_management'] = train_df['scheme_management'].fillna(\"missing\")\n",
    "train_df['permit'] = train_df['permit'].fillna(\"missing\")\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, categorical_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What options exist for category lumping in Python? I'm thinking of the forcats package in R. Can we do useful lumping with `funder` and `installer`? These feel like they might have decent predictive value.\n",
    "\n",
    "The [siuba](https://github.com/machow/siuba) package is a Python port of dplyr. The author implemented forcats as a module in that package and is now moving it to its own package. See https://pypi.org/project/forcats-py/#description. I pip installed siuba to check it out. There's **NO** need for you to do so unless you want to try it out for yourself. I'll be providing the pre-processed dataset based on this data prep notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from siuba.dply.forcats import fct_lump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct_lump(train_df['funder'], n=10).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct_lump(train_df['installer'], n=10).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['funder'] = fct_lump(train_df['funder'], n=10)\n",
    "#train_df['installer'] = fct_lump(train_df['installer'], n=10)\n",
    "#train_df['scheme_management'] = fct_lump(train_df['scheme_management'], n=10)\n",
    "\n",
    "#train_df.loc[:, categorical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before moving on to model building\n",
    "\n",
    "The work we did above is pretty typical data prep work to get ready to build predictive models. However, Jupyter notebooks such as this one, while great for documenting our thinking and exploration, is not that great for automated workflows. So, I'll typically extract the relevant code and create a data prep script that I can rerun as needed to generate preprocessed data files for the next stage in our analysis. See `data_prep.py` for the code I ended up with for this dataset. You'll see that it ends with exporting to both csv and json files since I wanted to experiment with both formats.\n",
    "\n",
    "Again, you do **NOT** need to run it. I've created the files already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_prep.py\n",
    "import pandas as pd\n",
    "from siuba.dply.forcats import fct_lump\n",
    "\n",
    "\n",
    "train_x = pd.read_csv(\"./data/raw/train_x.csv\", parse_dates=['date_recorded'])\n",
    "train_y = pd.read_csv(\"./data/raw/train_y.csv\")\n",
    "test_x = pd.read_csv(\"./data/raw/test_x.csv\", parse_dates=['date_recorded'])\n",
    "\n",
    "dict_df = {\"train_x\": train_x,\n",
    "           \"test_x\": test_x}\n",
    "           \n",
    "for key in dict_df:\n",
    "\n",
    "    df = dict_df[key]\n",
    "\n",
    "    # Initialize list of columns to keep\n",
    "    cols_to_keep = df.columns.tolist()\n",
    "\n",
    "    to_drop = ['date_recorded', 'recorded_by']\n",
    "    cols_to_keep = [c for c in cols_to_keep if c not in to_drop]\n",
    "\n",
    "    # Geo cols to remove\n",
    "    cols_to_keep.remove('region_code')\n",
    "    cols_to_keep.remove('district_code')\n",
    "    cols_to_keep.remove('wpt_name')\n",
    "    cols_to_keep.remove('subvillage')\n",
    "    cols_to_keep.remove('ward')\n",
    "\n",
    "    # Aggregated cols to remove\n",
    "    aggs_to_drop = ['extraction_type', 'extraction_type_group', 'management', 'payment', 'water_quality',\n",
    "                   'quantity', 'source', 'source_type', 'waterpoint_type', 'scheme_name']\n",
    "\n",
    "    cols_to_keep = [c for c in cols_to_keep if c not in aggs_to_drop]\n",
    "\n",
    "    df = df.loc[:, cols_to_keep]\n",
    "\n",
    "\n",
    "    # Change booleans to strings\n",
    "    df[\"permit\"] = df[\"permit\"].astype(\"string\")\n",
    "    df[\"public_meeting\"] = df[\"public_meeting\"].astype(\"string\")\n",
    "\n",
    "    # Missing data - code as \"missing\" for categorical data\n",
    "    df['funder'] = df['funder'].fillna(\"missing\")\n",
    "    df['installer'] = df['installer'].fillna(\"missing\")\n",
    "    df['public_meeting'] = df['public_meeting'].fillna(\"missing\")\n",
    "    df['scheme_management'] = df['scheme_management'].fillna(\"missing\")\n",
    "    df['permit'] = df['permit'].fillna(\"missing\")\n",
    "\n",
    "\n",
    "    # Factor level lumping\n",
    "\n",
    "    df['funder'] = fct_lump(df['funder'], n=10)\n",
    "    df['installer'] = fct_lump(df['installer'], n=10)\n",
    "    df['scheme_management'] = fct_lump(df['scheme_management'], n=10)\n",
    "    \n",
    "    df.to_json(f\"data/{key}.json\", orient='records')\n",
    "    df.to_csv(f\"data/{key}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exploration\n",
    "In the process of putting this notebook together, I ran across numerous interesting and relevant Python packages. I'll mention two of them and you can explore them if you are interested.\n",
    "\n",
    "\n",
    "### HoloViews\n",
    "\n",
    "This is another data visualization package. It has a different philosophy than packages like matplotlib and Seaborn (at least that's what they say):\n",
    "\n",
    "> HoloViews helps you understand your data better, by letting you work seamlessly with both the data and its graphical representation.\n",
    "\n",
    "* https://holoviews.org/index.html\n",
    "* https://holoviews.org/getting_started/Introduction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyvtreat\n",
    "\n",
    "In the pcda class, one of the textbooks we use is *Practical Data Science with R* by Mount and Zumel. They created an R package called vtreat for preparing data for predictive modeling. Now it appears they've ported to Python.\n",
    "\n",
    "* https://github.com/WinVector/pyvtreat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aap]",
   "language": "python",
   "name": "conda-env-aap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
